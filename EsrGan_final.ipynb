{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywOFtERmZSJ6",
        "outputId": "360e4e8d-a869-4299-8d91-b3c86cc22135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Model created with 16.72M parameters\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f4a5d3e234ef>:256: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split: Train=640, Val=80, Test=80\n",
            "Starting training for 20 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20:   0%|          | 0/320 [00:00<?, ?it/s]<ipython-input-1-f4a5d3e234ef>:316: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1/20: 100%|██████████| 320/320 [09:12<00:00,  1.73s/it, loss=5.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating after epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:50<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 5.2515, PSNR: 7.74, SSIM: 0.0339\n",
            "New best model with PSNR: 7.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 320/320 [09:08<00:00,  1.71s/it, loss=0.478]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating after epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:49<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.4779, PSNR: 12.14, SSIM: 0.1593\n",
            "New best model with PSNR: 12.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 320/320 [09:07<00:00,  1.71s/it, loss=0.235]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Loss: 0.2345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 320/320 [09:08<00:00,  1.71s/it, loss=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating after epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:48<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Loss: 0.1254, PSNR: 16.13, SSIM: 0.3277\n",
            "New best model with PSNR: 16.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 320/320 [09:08<00:00,  1.71s/it, loss=0.0815]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Loss: 0.0815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 320/320 [09:06<00:00,  1.71s/it, loss=0.0553]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating after epoch 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:48<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Loss: 0.0553, PSNR: 24.50, SSIM: 0.7407\n",
            "New best model with PSNR: 24.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 320/320 [09:06<00:00,  1.71s/it, loss=0.0418]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Loss: 0.0418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 320/320 [09:08<00:00,  1.71s/it, loss=0.0348]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating after epoch 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:48<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Loss: 0.0348, PSNR: 27.67, SSIM: 0.8382\n",
            "New best model with PSNR: 27.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 320/320 [09:07<00:00,  1.71s/it, loss=0.0319]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Loss: 0.0319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 320/320 [09:06<00:00,  1.71s/it, loss=0.0272]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating after epoch 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:47<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Loss: 0.0272, PSNR: 28.77, SSIM: 0.8757\n",
            "New best model with PSNR: 28.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20:  61%|██████    | 195/320 [05:33<03:33,  1.71s/it, loss=0.0232]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from skimage.metrics import peak_signal_noise_ratio as compute_psnr\n",
        "from skimage.metrics import structural_similarity as compute_ssim\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False  # For performance\n",
        "    torch.backends.cudnn.benchmark = True       # For performance\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Save directory in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/esrgan_output2'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "os.makedirs(f\"{save_dir}/samples\", exist_ok=True)\n",
        "os.makedirs(f\"{save_dir}/checkpoints\", exist_ok=True)\n",
        "os.makedirs(f\"{save_dir}/inference\", exist_ok=True)\n",
        "\n",
        "# Model parameters - larger capacity for H100\n",
        "nf1 = 64    # Number of filters\n",
        "gc1 = 32    # Growth channels\n",
        "\n",
        "# Residual Dense Block\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, nf=nf1, gc=gc1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(nf+gc, gc, 3, 1, 1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(nf+2*gc, gc, 3, 1, 1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(nf+3*gc, gc, 3, 1, 1, bias=False)\n",
        "        self.conv5 = nn.Conv2d(nf+4*gc, nf, 3, 1, 1, bias=False)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, nf=nf1, gc=gc1):\n",
        "        super().__init__()\n",
        "        self.RDB1 = ResidualDenseBlock(nf, gc)\n",
        "        self.RDB2 = ResidualDenseBlock(nf, gc)\n",
        "        self.RDB3 = ResidualDenseBlock(nf, gc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.RDB3(self.RDB2(self.RDB1(x))) * 0.2 + x\n",
        "\n",
        "class GeneratorRRDB(nn.Module):\n",
        "    def __init__(self, in_nc=3, out_nc=3, nf=nf1, nb=23, gc=gc1):  # 23 blocks like real ESRGAN\n",
        "        super().__init__()\n",
        "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=False)\n",
        "        self.RRDB_trunk = nn.Sequential(*[RRDB(nf, gc) for _ in range(nb)])\n",
        "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=False)\n",
        "\n",
        "        # 4x upsampling (256->1024, close to 1048)\n",
        "        self.upsampling = nn.Sequential(\n",
        "            nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=False),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=False),\n",
        "            # nn.PixelShuffle(2),\n",
        "            # nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        # Final convolution\n",
        "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fea = self.conv_first(x)\n",
        "        trunk = self.RRDB_trunk(fea)\n",
        "        fea = fea + self.trunk_conv(trunk)\n",
        "        out = self.conv_last(self.upsampling(fea))\n",
        "        return out\n",
        "\n",
        "# Custom Dataset for DIV2K\n",
        "class DIV2KSRDataset(Dataset):\n",
        "    def __init__(self, split=\"train\", cache=True, hr_size=512, lr_size=256):\n",
        "        self.dataset = load_dataset(\"eugenesiow/Div2k\", split=split, cache_dir=\"./div2k_data\")\n",
        "        self.cache = cache\n",
        "        self.cached_data = {}\n",
        "        self.hr_size = hr_size\n",
        "        self.lr_size = lr_size\n",
        "\n",
        "        # HR transform - target size 1048x1048\n",
        "        self.hr_transform = transforms.Compose([\n",
        "            transforms.Resize((hr_size, hr_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        # LR transform - input size 256x256\n",
        "        self.lr_transform = transforms.Compose([\n",
        "            transforms.Resize((lr_size, lr_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.cache and idx in self.cached_data:\n",
        "            return self.cached_data[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = self.dataset[idx][\"hr\"]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Generate HR and LR images\n",
        "        hr = self.hr_transform(img)\n",
        "        lr = self.lr_transform(img)\n",
        "\n",
        "        if self.cache:\n",
        "            self.cached_data[idx] = (lr, hr)\n",
        "\n",
        "        return lr, hr\n",
        "\n",
        "# Convert tensor to numpy for visualization\n",
        "def tensor_to_numpy(tensor):\n",
        "    \"\"\"Convert a torch tensor to numpy array for visualization\"\"\"\n",
        "    img = tensor.detach().cpu().numpy().transpose(1, 2, 0)\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "# Calculate PSNR between two images\n",
        "def calculate_psnr(img1, img2):\n",
        "    \"\"\"Calculate PSNR between two images (numpy arrays)\"\"\"\n",
        "    img1_np = tensor_to_numpy(img1)\n",
        "    img2_np = tensor_to_numpy(img2)\n",
        "    return compute_psnr(img1_np, img2_np, data_range=1.0)\n",
        "\n",
        "# Calculate SSIM between two images\n",
        "def calculate_ssim(img1, img2):\n",
        "    \"\"\"Calculate SSIM between two images (numpy arrays)\"\"\"\n",
        "    img1_np = tensor_to_numpy(img1)\n",
        "    img2_np = tensor_to_numpy(img2)\n",
        "    return compute_ssim(img1_np, img2_np, data_range=1.0, channel_axis=2, multichannel=True)\n",
        "\n",
        "# Visualize results during training\n",
        "def visualize_samples(model, data_loader, device, epoch, save_dir, num_samples=3):\n",
        "    \"\"\"Visualize and save sample results\"\"\"\n",
        "    model.eval()\n",
        "    os.makedirs(f\"{save_dir}/samples/epoch_{epoch}\", exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (lr, hr) in enumerate(data_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            lr, hr = lr.to(device), hr.to(device)\n",
        "            sr = model(lr)\n",
        "\n",
        "            # Process only the first image in the batch\n",
        "            lr_np = tensor_to_numpy(lr[0])\n",
        "            hr_np = tensor_to_numpy(hr[0])\n",
        "            sr_np = tensor_to_numpy(sr[0])\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr_val = compute_psnr(sr_np, hr_np, data_range=1.0)\n",
        "            ssim_val = compute_ssim(sr_np, hr_np, data_range=1.0, channel_axis=2, multichannel=True)\n",
        "\n",
        "            # Create visualization\n",
        "            plt.figure(figsize=(18, 6))\n",
        "\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(lr_np)\n",
        "            plt.title(f'Low Resolution ({lr_np.shape[0]}x{lr_np.shape[1]})')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(sr_np)\n",
        "            plt.title(f'Super Resolution - PSNR: {psnr_val:.2f}, SSIM: {ssim_val:.4f}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(hr_np)\n",
        "            plt.title(f'High Resolution ({hr_np.shape[0]}x{hr_np.shape[1]})')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{save_dir}/samples/epoch_{epoch}/sample_{i}.png\", dpi=200)\n",
        "            plt.close()\n",
        "\n",
        "# Evaluate model on validation or test set\n",
        "def evaluate_model(model, data_loader, device, desc=\"Evaluating\"):\n",
        "    \"\"\"Evaluate model and compute metrics\"\"\"\n",
        "    model.eval()\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for lr, hr in tqdm(data_loader, desc=desc):\n",
        "            lr, hr = lr.to(device), hr.to(device)\n",
        "            sr = model(lr)\n",
        "\n",
        "            # Calculate metrics for each image in batch\n",
        "            for i in range(sr.size(0)):\n",
        "                psnr_val = calculate_psnr(sr[i], hr[i])\n",
        "                ssim_val = calculate_ssim(sr[i], hr[i])\n",
        "\n",
        "                psnr_values.append(psnr_val)\n",
        "                ssim_values.append(ssim_val)\n",
        "\n",
        "    avg_psnr = np.mean(psnr_values)\n",
        "    avg_ssim = np.mean(ssim_values)\n",
        "\n",
        "    return avg_psnr, avg_ssim\n",
        "\n",
        "def train_esrgan():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "    # Create model\n",
        "    model = GeneratorRRDB().to(device)\n",
        "    print(f\"Model created with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, betas=(0.9, 0.99))\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    # Load and split dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    full_dataset = DIV2KSRDataset(split=\"train\", cache=True, hr_size=512, lr_size=256)\n",
        "\n",
        "    # Split dataset into train, val, test (80%, 10%, 10%)\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = int(0.1 * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset split: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 2  # Adjust based on GPU memory\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                             num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False,\n",
        "                             num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
        "                             num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Sample visualization loader (3 fixed samples from validation)\n",
        "    vis_loader = DataLoader(val_dataset, batch_size=1, shuffle=False,\n",
        "                            num_workers=1, pin_memory=True)\n",
        "\n",
        "    # Training variables\n",
        "    num_epochs = 20\n",
        "    best_psnr = 0\n",
        "    train_losses = []\n",
        "    val_psnrs = []\n",
        "    val_ssims = []\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        # Training phase\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for lr, hr in pbar:\n",
        "            # Move data to device\n",
        "            lr, hr = lr.to(device), hr.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if scaler is not None:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    sr = model(lr)\n",
        "                    loss = criterion(sr, hr)\n",
        "\n",
        "                # Mixed precision backward pass\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Standard precision training\n",
        "                sr = model(lr)\n",
        "                loss = criterion(sr, hr)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "            pbar.set_postfix({'loss': epoch_loss / batch_count})\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        avg_loss = epoch_loss / batch_count\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Validation phase (every 2 epochs to save time)\n",
        "        if (epoch + 1) % 2 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
        "            print(f\"Validating after epoch {epoch+1}...\")\n",
        "\n",
        "            # Compute validation metrics\n",
        "            val_psnr, val_ssim = evaluate_model(model, val_loader, device, desc=\"Validation\")\n",
        "            val_psnrs.append(val_psnr)\n",
        "            val_ssims.append(val_ssim)\n",
        "\n",
        "            print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_psnr > best_psnr:\n",
        "                best_psnr = val_psnr\n",
        "                print(f\"New best model with PSNR: {best_psnr:.2f}\")\n",
        "                torch.save(model.state_dict(), f\"{save_dir}/checkpoints/esrgan_best.pth\")\n",
        "\n",
        "            # Visualize samples\n",
        "            visualize_samples(model, vis_loader, device, epoch+1, save_dir)\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, f\"{save_dir}/checkpoints/esrgan_epoch{epoch+1}.pth\")\n",
        "\n",
        "    # Final model save\n",
        "    torch.save(model.state_dict(), f\"{save_dir}/checkpoints/esrgan_final.pth\")\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('L1 Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(val_psnrs)\n",
        "    plt.title('Validation PSNR')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PSNR (dB)')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(val_ssims)\n",
        "    plt.title('Validation SSIM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/training_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Training completed. Testing best model...\")\n",
        "\n",
        "    # Load best model for testing\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/checkpoints/esrgan_best.pth\"))\n",
        "\n",
        "    # Test evaluation\n",
        "    test_psnr, test_ssim = evaluate_model(model, test_loader, device, desc=\"Testing\")\n",
        "    print(f\"Test Results - PSNR: {test_psnr:.2f}, SSIM: {test_ssim:.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    with open(f\"{save_dir}/test_results.txt\", \"w\") as f:\n",
        "        f.write(f\"Test PSNR: {test_psnr:.4f}\\n\")\n",
        "        f.write(f\"Test SSIM: {test_ssim:.4f}\\n\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def inference(model_path, test_dataset, save_dir):\n",
        "    \"\"\"Run inference on test dataset and visualize results\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model\n",
        "    model = GeneratorRRDB().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Create data loader for test samples\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Create directory for inference results\n",
        "    os.makedirs(f\"{save_dir}/inference\", exist_ok=True)\n",
        "\n",
        "    # Run inference on test samples\n",
        "    with torch.no_grad():\n",
        "        for i, (lr, hr) in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
        "            if i >= 20:  # Generate 20 samples\n",
        "                break\n",
        "\n",
        "            lr, hr = lr.to(device), hr.to(device)\n",
        "\n",
        "            # Generate super-resolution image\n",
        "            sr = model(lr)\n",
        "\n",
        "            # Calculate metrics\n",
        "            lr_np = tensor_to_numpy(lr[0])\n",
        "            hr_np = tensor_to_numpy(hr[0])\n",
        "            sr_np = tensor_to_numpy(sr[0])\n",
        "\n",
        "            psnr_val = compute_psnr(sr_np, hr_np, data_range=1.0)\n",
        "            ssim_val = compute_ssim(sr_np, hr_np, data_range=1.0, channel_axis=2, multichannel=True)\n",
        "\n",
        "            # Create visualization\n",
        "            plt.figure(figsize=(18, 8))\n",
        "\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(lr_np)\n",
        "            plt.title(f'Low Resolution ({lr_np.shape[0]}x{lr_np.shape[1]})')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(sr_np)\n",
        "            plt.title(f'Super Resolution - PSNR: {psnr_val:.2f}, SSIM: {ssim_val:.4f}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(hr_np)\n",
        "            plt.title(f'High Resolution ({hr_np.shape[0]}x{hr_np.shape[1]})')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{save_dir}/inference/sample_{i+1}.png\", dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    trained_model = train_esrgan()\n",
        "\n",
        "    # Create test dataset\n",
        "    test_dataset = DIV2KSRDataset(split=\"validation\", cache=True, hr_size=1048, lr_size=256)\n",
        "\n",
        "    # Run inference\n",
        "    inference(\n",
        "        model_path=f\"{save_dir}/checkpoints/esrgan_best.pth\",\n",
        "        test_dataset=test_dataset,\n",
        "        save_dir=save_dir\n",
        "    )\n",
        "\n",
        "    print(f\"All results saved to {save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "metadata": {
        "id": "rdwJYwoBaYy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6JHDqEnast_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}